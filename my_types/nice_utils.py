
import os
import akshare as ak
import pandas as pd
from datetime import datetime,date
import csv
balanced_path=os.path.join(os.getcwd(),'balanced')
Equity_path=os.path.join(os.getcwd(),'Equity')
index_path=os.path.join(os.getcwd(),'index')
Qdii_path=os.path.join(os.getcwd(),'Qdii')
# ['æ··åˆå‹-åè‚¡', 'æ··åˆå‹-å®è§‚ç­–ç•¥', 'æ··åˆå‹-çµæ´»é…ç½®', 'æ··åˆå‹-åå€º', 'æ··åˆå‹-è‚¡ç¥¨å¯¹å†²', 
#  'æ··åˆå‹-äº‹ä»¶é©±åŠ¨', 'æ··åˆå‹-è‚¡å€ºå¹³è¡¡', 'QDII-æ··åˆ', 'FOF-åå€ºæ··åˆ', 'FOF-åè‚¡æ··åˆ', 'æ··åˆå‹-å…¶ä»–']



def save_to_folder(df, base_path, file_name):
    try:
        file_path = os.path.join(base_path, file_name)
        print(f"æˆåŠŸä¿å­˜ {file_name} åˆ° {base_path} æ–‡ä»¶å¤¹")
        if os.path.exists(file_path):
            pass
        else:
            df.to_csv(file_path, index=False, encoding='utf-8')
    except Exception as e:
        print(f"ä¿å­˜æ–‡ä»¶ {file_name} æ—¶å‡ºé”™: {e}")

def where_to_go(code):
    "æ ¹æ®åŸºé‡‘ä»£ç æ‹¿åˆ°æ•°æ®ï¼Œä¿å­˜åˆ°æ–‡ä»¶å¤¹"
    print(f'å¼€å§‹å¤„ç†{code}')
    try:
        fund_info_df = ak.fund_individual_basic_info_xq(symbol=code)
        fund_type = fund_info_df[fund_info_df['item'] == 'åŸºé‡‘ç±»å‹']['value'].iloc[0]
        df = ak.fund_open_fund_info_em(symbol=code, indicator="ç´¯è®¡å‡€å€¼èµ°åŠ¿")
        file_name = f"{code}.csv"
        if 'æ··åˆ' in fund_type:
            save_to_folder(df, balanced_path, file_name)
            return
        elif 'è‚¡ç¥¨' in fund_type:
            save_to_folder(df, Equity_path, file_name)
            return
        elif 'æŒ‡æ•°' in fund_type:
            save_to_folder(df, index_path, file_name)
            return
        elif 'QDII' in fund_type:
            save_to_folder(df, Qdii_path, file_name)
            return
        else:
            print(f"æœªæ‰¾åˆ°åŒ¹é…çš„åŸºé‡‘ç±»å‹: {fund_type}")
    except Exception as e:
        print(f"æ²¡æœ‰æ‰¾åˆ°æˆ–å¤±è´¥ {code}: {e}")
        return



def update_files(path, cache_path, progress_callback=None):
    """
    æ›´æ–°æ‰€æœ‰æ–‡ä»¶, æ”¯æŒæ–­ç‚¹ç»­ä¼ ã€‚æ¯ä¸ªæ–‡ä»¶æ›´æ–°åè°ƒç”¨ progress_callback æ›´æ–°è¿›åº¦ã€‚
    """
    today = date.today()
    
    # --- 1. è·¯å¾„å’Œæ–‡ä»¶å‡†å¤‡ ---
    try:
        all_files = os.listdir(path)
    except FileNotFoundError:
        # å¦‚æœpathä¸å­˜åœ¨ï¼Œè¯´æ˜åˆ†ç»„ç›®å½•æœ‰é—®é¢˜ï¼Œç›´æ¥è¿”å›
        print(f"é”™è¯¯ï¼šåˆ†ç»„è·¯å¾„ä¸å­˜åœ¨æˆ–æ— æ³•è®¿é—®ï¼š{path}")
        return

    csv_files = [file for file in all_files if file.endswith('.csv')]
    total_len = len(csv_files)
    count = 1

    try:
        # å°è¯•è¯»å–ç¼“å­˜æ–‡ä»¶ï¼Œå¦‚æœæ–‡ä»¶ä¸å­˜åœ¨ï¼Œå°†ç”±è°ƒç”¨è€…å¤„ç† FileNotFoundError
        cache_df = pd.read_csv(cache_path)
    except FileNotFoundError:
        # æ­¤å¤„åªæ˜¯æ‰“å°é”™è¯¯ï¼Œä½†ç¨‹åºä¼šç»§ç»­è¿è¡Œï¼Œå¯èƒ½ä¼šåœ¨åé¢çš„ä»£ç ä¸­å¤±è´¥
        print(f"è‡´å‘½é”™è¯¯ï¼šæœªæ‰¾åˆ°ç¼“å­˜æ–‡ä»¶ {cache_path}ï¼Œæ— æ³•è¿›è¡Œæ–­ç‚¹ç»­ä¼ ã€‚")
        return

    # --- 2. éå†å¹¶æ›´æ–°æ–‡ä»¶ ---
    for single in csv_files:
        file_path = os.path.join(path, single)
        fund_code = single.split('.')[0]
        
        try:
            # ğŸŒŸ ä¿®å¤æ ¸å¿ƒé€»è¾‘ï¼šå®‰å…¨åœ°ä»ç¼“å­˜ä¸­è·å–æ—¥æœŸ ğŸŒŸ
            # 1. æŸ¥æ‰¾åŒ¹é…çš„æ—¥æœŸ Series
            date_series = cache_df.loc[cache_df['path'] == file_path, 'latest_date']

            # 2. æ£€æŸ¥ Series é•¿åº¦ï¼Œå®‰å…¨æå–å•å€¼
            if len(date_series) == 1:
                # æ‰¾åˆ°å”¯ä¸€åŒ¹é…é¡¹ï¼Œå®‰å…¨æå–å€¼
                cached_date = str(date_series.iloc[0]) 
            elif len(date_series) == 0:
                # ç¼“å­˜ä¸­æ²¡æœ‰è¯¥æ–‡ä»¶çš„è®°å½•ï¼Œè§†ä¸ºä»æœªæ›´æ–°è¿‡
                cached_date = '1970-01-01' 
            else:
                # è‡´å‘½æ•°æ®é”™è¯¯ï¼šè·¯å¾„é‡å¤
                print(f"è‡´å‘½æ•°æ®é”™è¯¯ï¼šç¼“å­˜ä¸­å­˜åœ¨å¤šæ¡è®°å½•åŒ¹é…è·¯å¾„ {file_path}ï¼Œè·³è¿‡æ›´æ–°ã€‚")
                count += 1
                if progress_callback:
                    progress_callback(count, total_len)
                continue
            
            # --- 3. æ¯”è¾ƒæ—¥æœŸå’Œæ›´æ–°é€»è¾‘ ---
            
            if cached_date == today.strftime('%Y-%m-%d'):
                # ç¼“å­˜æ—¥æœŸç­‰äºä»Šå¤©ï¼Œè·³è¿‡
                print(f'{single} ç¼“å­˜ä¸­å·²ç»æ˜¯æœ€æ–°ï¼Œè·³è¿‡æ›´æ–° ({count}/{total_len})ã€‚')
                
            else:
                # éœ€è¦è¿›è¡Œæ•°æ®è¯·æ±‚å’Œæ›´æ–°
                print(f'{single} æ­£åœ¨è¿›è¡Œç½‘ç»œè¯·æ±‚...')
                
                # ç½‘ç»œè¯·æ±‚
                data = ak.fund_open_fund_info_em(symbol=fund_code, indicator="ç´¯è®¡å‡€å€¼èµ°åŠ¿")
                
                # æ•°æ®å¤„ç†
                if data is None or data.empty:
                    print(f'{single} è­¦å‘Šï¼šakshareè¿”å›ç©ºæ•°æ®ï¼Œè·³è¿‡å†™å…¥ ({count}/{total_len})ã€‚')
                    count += 1
                    if progress_callback:
                        progress_callback(count, total_len)
                    continue

                data["å‡€å€¼æ—¥æœŸ"] = pd.to_datetime(data['å‡€å€¼æ—¥æœŸ'])
                latest_date = data['å‡€å€¼æ—¥æœŸ'].max()
                latest_date_str = latest_date.strftime('%Y-%m-%d')
                
                # æ£€æŸ¥æœ€æ–°æ—¥æœŸæ˜¯å¦ä¸ç¼“å­˜æ—¥æœŸä¸€è‡´ï¼ˆæ•°æ®æºå°šæœªæ›´æ–°ï¼‰
                if latest_date_str == cached_date:
                    print(f'{single} æ•°æ®æºæœªæ›´æ–°ï¼Œç¼“å­˜å·²æ˜¯æœ€æ–°ï¼Œè·³è¿‡å†™å…¥ç¼“å­˜ ({count}/{total_len})ã€‚')
                else:
                    # å†™å…¥æ•°æ®å’Œæ›´æ–°ç¼“å­˜
                    output_path = os.path.join(path, single)
                    data.to_csv(output_path, index=False)
                    
                    # æ›´æ–°ç¼“å­˜ DataFrame
                    # ä½¿ç”¨å¸ƒå°”ç´¢å¼•æŸ¥æ‰¾å¹¶æ›´æ–° latest_date
                    cache_df.loc[cache_df['path'] == file_path, 'latest_date'] = latest_date_str
                    
                    # å†™å…¥ç¼“å­˜æ–‡ä»¶
                    cache_df.to_csv(cache_path, index=False)
                    
                    print(f'{single} æ›´æ–°æˆåŠŸ ({count}/{total_len})ï¼Œæ›´æ–°æ—¥æœŸä¸º {latest_date_str}ã€‚ç¼“å­˜å·²åŒæ­¥å†™å…¥ã€‚')
                    
            # --- 4. è¿›åº¦æ›´æ–° ---
            count += 1
            if progress_callback:
                progress_callback(count, total_len)
                
        except Exception as e:
            # æ•è· akshare ç½‘ç»œé”™è¯¯ã€æ–‡ä»¶å†™å…¥é”™è¯¯ç­‰
            print(f"æ›´æ–°å¤±è´¥ {fund_code}: {e}")
            count += 1
            if progress_callback:
                progress_callback(count, total_len)
            continue
            
    print('æ‰€æœ‰æ–‡ä»¶æ›´æ–°å¤„ç†å®Œæˆï¼')

def de_dupulicate(path):
    """
    è¯»å–æŒ‡å®šè·¯å¾„çš„ CSV æ–‡ä»¶ï¼ˆç¼“å­˜æ–‡ä»¶ï¼‰ï¼Œæ ¹æ® 'path' åˆ—å»é‡ã€‚

    Args:
        path (str): CSV æ–‡ä»¶çš„å®Œæ•´è·¯å¾„ï¼ˆå³ç¼“å­˜æ–‡ä»¶çš„åœ°å€ï¼‰ã€‚
    """
    if not os.path.exists(path):
        print(f"é”™è¯¯ï¼šæœªæ‰¾åˆ°æ–‡ä»¶ï¼Œè·¯å¾„ä¸º {path}")
        return
    try:
        cache_df = pd.read_csv(path)
        if 'path' not in cache_df.columns:
            print(f"é”™è¯¯ï¼šæ–‡ä»¶ {path} ä¸­æœªæ‰¾åˆ° 'path' åˆ—ï¼Œæ— æ³•å»é‡ã€‚")
            return
        initial_rows = len(cache_df)
        df_deduplicated = cache_df.drop_duplicates(subset=['path'], keep='last')
        final_rows = len(df_deduplicated)
        df_deduplicated.to_csv(path, index=False)
        print(f"æ–‡ä»¶å»é‡æˆåŠŸï¼š{path}")
        print(f" - åŸå§‹è¡Œæ•°: {initial_rows}")
        print(f" - å»é‡åè¡Œæ•°: {final_rows}")
        print(f" - ç§»é™¤äº†é‡å¤è¡Œæ•°: {initial_rows - final_rows}")
    except pd.errors.EmptyDataError:
        print(f"è­¦å‘Šï¼šæ–‡ä»¶ {path} ä¸ºç©ºï¼Œæ— éœ€å¤„ç†ã€‚")
    except Exception as e:
        print(f"å¤„ç†æ–‡ä»¶ {path} æ—¶å‘ç”ŸæœªçŸ¥é”™è¯¯: {e}")




def exam(path):
    """æŸ¥çœ‹æ‰€æœ‰æ–‡ä»¶çš„åŸºé‡‘ç±»å‹"""
    all_files = os.listdir(path)
    csv_files = [file for file in all_files if file.endswith('.csv')]
    total_len=len(csv_files)
    count=1
    storage=[]
    for single in csv_files:
        fund_code = single.split('.')[0]
        try:
            print(f'å¤„ç†{total_len}ä¸­çš„ç¬¬{count}ä¸ª')
            fund_info_df = ak.fund_individual_basic_info_xq(symbol=fund_code)
            fund_type = fund_info_df[fund_info_df['item'] == 'åŸºé‡‘ç±»å‹']['value'].iloc[0]
            if fund_type not in storage:
                storage.append(fund_type)
            else:
                pass
            count+=1
        except Exception as e:
            print(f"å¤±è´¥ {fund_code}: {e}")
    print(storage)
            
def flush(path):
    """å±é™©æ–¹æ³•"""
    all_files = os.listdir(path)
    csv_files = [file for file in all_files if file.endswith('.csv')]
    total_len=len(csv_files)
    count=1
    del_count=0
    for single in csv_files:
        fund_code = single.split('.')[0]
        try:
            fund_info_df = ak.fund_individual_basic_info_xq(symbol=fund_code)
            fund_type = fund_info_df[fund_info_df['item'] == 'åŸºé‡‘ç±»å‹']['value'].iloc[0]
            if'FOF'in fund_type:
                single_path=os.path.join(path,single)
                os.remove(single_path)
                print(f'{single}åˆ é™¤æˆåŠŸ,{total_len}ä¸­çš„ç¬¬{count}ä¸ª')
                del_count+=1
            else:
                print(f'{fund_type}æœªåˆ é™¤ï¼Œ{total_len}ä¸­çš„ç¬¬{count}ä¸ª')
                pass
            count+=1
        except Exception as e:
            print(f"å¤±è´¥ {fund_code}: {e}")
    print(f'ä»»åŠ¡å®Œæˆï¼ŒæˆåŠŸåˆ é™¤{del_count}ä¸ªæ–‡ä»¶')

def recover():
    list=['000216', '000217', '000218', '000307', '000929', '000930', '002610', '002611', '002963', '004253', '007910', '007911', '007937', '007938', '008142', '008143', '008701', '008702', '008827', '008828', '008986', '008987', '009033', '009034', '009198', '009477', '009478', '009504', '009505', '014661', '014662', '016581', '016582', '018391', '018392', '019005']
    count=0
    for single in list:
        try:
            df=ak.fund_open_fund_info_em(symbol=single, indicator="ç´¯è®¡å‡€å€¼èµ°åŠ¿")
            file_name = f"{single}.csv"
            full_path=os.path.join(Qdii_path,file_name)
            print(full_path)
            df.to_csv(full_path,index=False)
            print(f'{single}å†™å…¥æˆåŠŸ')
            count+=1
        except Exception as e:
            print(f"å¤±è´¥ {single}: {e}")
    print(f'ä»»åŠ¡å®Œæˆï¼ŒæˆåŠŸå†™å…¥{count}ä¸ªæ–‡ä»¶')

def flush_those_outdated():
    """å±é™©æ–¹æ³•"""
    mapping_path = r"A:\projects\money2\mapping\mapping_latestdate.csv"
    df = pd.read_csv(mapping_path)
    df['latest_date'] = pd.to_datetime(df['latest_date'])
    threshold_date = datetime(2025, 9, 1)
    files_to_delete = df[df['latest_date'] < threshold_date]
    num=0
    for index, row in files_to_delete.iterrows():
        file_path = row['path']
        if os.path.exists(file_path):
            try:
                os.remove(file_path)  # åˆ é™¤æ–‡ä»¶
                print(f"å·²åˆ é™¤æ–‡ä»¶: {file_path}")
                num+=1
            except Exception as e:
                print(f"åˆ é™¤æ–‡ä»¶å¤±è´¥: {file_path}ï¼Œé”™è¯¯: {e}")
        else:
            print(f"æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
    df = df[df['latest_date'] >= threshold_date]
    df.to_csv(mapping_path, index=False)
    print(f"ä»»åŠ¡å®Œæˆï¼ŒæˆåŠŸåˆ é™¤{num}ä¸ªæ–‡ä»¶")



def collect_csv_files():   
    csv_files = []
    for file in os.listdir(balanced_path):
        if file.endswith('.csv'):
            file=file.split('.')[0]
            if file not in csv_files:
                csv_files.append(file)
    for file in os.listdir(Equity_path):
        if file.endswith('.csv'):
            file=file.split('.')[0]
            if file not in csv_files:
                csv_files.append(file)
    for file in os.listdir(Qdii_path):
        if file.endswith('.csv'):
            file=file.split('.')[0]
            if file not in csv_files:
                csv_files.append(file)
    for file in os.listdir(index_path):
        if file.endswith('.csv'):
            file=file.split('.')[0]
            if file not in csv_files:
                csv_files.append(file)
    if csv_files:
        with open('collect_list.csv', mode='w', newline='') as file:
            writer = csv.writer(file)
            writer.writerow(['File Name'])  # å†™å…¥åˆ—æ ‡é¢˜
            for csv_file in csv_files:
                writer.writerow([csv_file])  # å†™å…¥æ¯ä¸ªæ–‡ä»¶å
        print(f'å…±æ‰¾åˆ° {len(csv_files)} ä¸ª CSV æ–‡ä»¶ï¼Œå¹¶å·²ä¿å­˜åˆ° collect_list.csv')
    else:
        print('æœªæ‰¾åˆ°ä»»ä½• CSV æ–‡ä»¶')

if __name__ == "__main__":
       for code in range(28000,30000):
            zfilledcode=str(code).zfill(6)
            where_to_go(zfilledcode)
        
        
